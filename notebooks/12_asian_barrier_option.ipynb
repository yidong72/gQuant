{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Option Pricing and Deep Learning model in gQuant\n",
    "\n",
    "\n",
    "The Black–Scholes model can efficiently be used for pricing “plain vanilla” options with the European exercise rule. Options like the Barrier option and Basket option have a complicated structure with no simple analytical solution. The Monte Carlo simulation is an effective way to price them. Traditionally, Monte Carlo pricing is done in the C/C++ CUDA code.  In this [developer blog](https://developer.nvidia.com/blog/accelerating-python-for-exotic-option-pricing/), I explored how to use Python GPU libraries to achieve the state-of-the-art performance in the domain of exotic option pricing. The Monte Carlo simulation method I used has one limitations that it cannot handle the continuous maturity time, which is critical to calculate the Greek Theta.  \n",
    "\n",
    "Recently, Huge and Savine introduced a novel regularization for training fast, accurate pricing in a [paper](https://arxiv.org/pdf/2005.02347.pdf). Inspired by this method, in this notebook we are going to:\n",
    "\n",
    "    1. Handle the continuous maturity time T.     \n",
    "    2. Implement the differential regularization for the example Asian Barrier option\n",
    "    3. Show how we do HPC (Monte Carlo simulation) and deep learning in gQuant way. \n",
    "    \n",
    "Without loss of generality, we use the Asian Barrier Option as an example. The Asian Barrier Option is a mixture of the Asian Option and the Barrier Option. The derivative price depends on the average of underlying Asset Price S, the Strike Price K, and the Barrier Price B.  Use the Down-and-Out Call Discretized Asian Barrier Option as an example. \n",
    "\n",
    "    The option is void if the average price of the underlying asset goes below the barrier. \n",
    "    The asset Spot Price S is usually modeled as Geometric Brownian motion, which has three free parameters: Spot Price, Percent Volatility, and Percent Drift. \n",
    "    The price of the option is the expected profit at the maturity discount to the current value. \n",
    "    The path-dependent nature of the option makes an analytic solution of the option price impossible. \n",
    "\n",
    "This is a good sample option for pricing using the Monte Carlo simulation. \n",
    "\n",
    "As a reresher, let's first run Monte Carlo simulation for Option pricing using the method introduced in the [developer blog](https://developer.nvidia.com/blog/accelerating-python-for-exotic-option-pricing/). We choose to price the example Asian Barrier Option:\n",
    "\n",
    "    Maturity (T): 1.1 year\n",
    "    Spot (S) : 120\n",
    "    Strike (K): 110\n",
    "    Volatility (sigma): 35.0 %\n",
    "    Risk Free Rate (r): 5.0 %\n",
    "    Stock Drift Rate (mu): 10.0 %\n",
    "    Barrier (B): 100\n",
    "\n",
    "To handle continuous maturity time, we fix the number of steps per year and do the fractional step for the last step. Import the libraries and define the option parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 278\n"
     ]
    }
   ],
   "source": [
    "import cupy\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import numba\n",
    "from numba import cuda\n",
    "from numba import njit\n",
    "from numba import prange\n",
    "import cudf\n",
    "cupy.cuda.set_allocator(None)\n",
    "#110.0, 100.0, 120.0, 0.35, 0.1, 0.05\n",
    "N_PATHS = 8192000\n",
    "Y_STEPS = 252 # constant, number of steps per year\n",
    "T = 1.1 # time, unit 1 year\n",
    "K = 110.0 # Strike price\n",
    "B = 100.0 # barrier price\n",
    "S0 = 120.0 # initial stock price \n",
    "sigma = 0.35 # stock annual volatility \n",
    "mu = 0.1 # stock annual return\n",
    "r = 0.05 # stock annual interest rate\n",
    "N_STEPS = int(np.ceil(T * Y_STEPS))\n",
    "print('steps', N_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "allocate GPU arrays for random numbers and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "randoms_gpu = cupy.random.normal(0, 1, N_PATHS * N_STEPS, dtype=cupy.float32)\n",
    "output =  np.zeros(N_PATHS, dtype=np.float32)\n",
    "doutput =  np.zeros(N_PATHS*6, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the Numba kernel that we use to run simulation for each of the path. Note, the last step is handled specially to account for the continuous maturity time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def numba_gpu_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS):\n",
    "    # ii - overall thread index\n",
    "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
    "    tmp1 = mu/Y_STEPS\n",
    "    tmp2 = math.exp(-r*T)\n",
    "    tmp3 = math.sqrt(1.0/Y_STEPS)\n",
    "    running_average = 0.0\n",
    "    for i in range(ii, N_PATHS, stride):\n",
    "        s_curr = S0\n",
    "        for n in range(N_STEPS):\n",
    "            if n == N_STEPS - 1:\n",
    "                delta_t = T - n/Y_STEPS\n",
    "                tmp1 = delta_t * mu\n",
    "                tmp3 = math.sqrt(delta_t)                \n",
    "            s_curr += tmp1 * s_curr + sigma*s_curr*tmp3*d_normals[i + n * N_PATHS]\n",
    "            running_average += (s_curr - running_average) / (n + 1.0)\n",
    "\n",
    "            if running_average <= B:\n",
    "                break\n",
    "        payoff = running_average - K if running_average>K else 0\n",
    "        d_s[i] = tmp2 * payoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the simulation and benchmark the computation time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 0.37772631645202637 v 19.44895\n"
     ]
    }
   ],
   "source": [
    "number_of_threads = 256\n",
    "number_of_blocks = (N_PATHS-1) // number_of_threads + 1\n",
    "output = cupy.zeros(N_PATHS, dtype=cupy.float32)\n",
    "numba_gpu_barrier_option[(number_of_blocks,), (number_of_threads,)](output, np.float32(T), np.float32(K), \n",
    "                    np.float32(B), np.float32(S0), \n",
    "                    np.float32(sigma), np.float32(mu), \n",
    "                    np.float32(r), randoms_gpu, N_STEPS, N_PATHS)\n",
    "s = time.time()\n",
    "numba_gpu_barrier_option[(number_of_blocks,), (number_of_threads,)](output, np.float32(T), np.float32(K), \n",
    "                    np.float32(B), np.float32(S0), \n",
    "                    np.float32(sigma), np.float32(mu), \n",
    "                    np.float32(r), randoms_gpu, N_STEPS, N_PATHS)\n",
    "v = output.mean()\n",
    "cuda.synchronize()\n",
    "e = time.time()\n",
    "print('time', e-s, 'v', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic adjoint differentiation(AAD) can be applied in the Monte Carlo simulation to calculate the Greeks accurately and efficiently according to \n",
    "the [paper](https://arxiv.org/pdf/2005.02347.pdf). We need to do some derivation to find the formular of pathwise differentials.\n",
    "\n",
    "The option parameters are $T$, $K$, $S_0$, $\\sigma$, $\\mu$, $r$. For simplicity, we define $\\theta=(T, K, S_0, \\sigma, \\mu, r)$.\n",
    "\n",
    "The option price is computed by $$ p = E(f_i(\\theta)) = \\frac{1}{N}\\sum_i f_i$$, where $f_i$ is the option value at the exercise time for the $i^{th}$ path. The Greeks are the first-order differentiation with respect to $\\theta$: \n",
    "\n",
    "$$\\nabla_{\\theta} p = \\frac{1}{N}\\sum_i \\nabla_{\\theta} f_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on the calculation of gradient of $f_i(\\theta)$. $f_i$ is calculated by Monte Carlo simulation method. Break it down into individual time steps. Without loss of generality, we drop the index $i$ here.\n",
    "\n",
    "$$    \\nabla_{\\theta} f = \n",
    "\\begin{cases}\n",
    "    \\nabla_{\\theta} (a_n(\\theta) - K)  & \\text{if } a_n\\geq K\\\\\n",
    "    (0,0,0,0,0,0)              & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where the moving average $a_n$ at step $n$ is\n",
    "\n",
    "$$a_n = g(a_{n-1}, s_{n}) = a_{n-1} + \\frac{s_{n} - a_{n-1}}{n + 1.0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of $a_n$:\n",
    "$$ \\nabla_{\\theta} a_n = \\frac{\\partial g} {\\partial a_{n-1}} \\nabla_{\\theta} a_{n-1} + \\frac{\\partial g} {\\partial s_{n}} \\nabla_{\\theta} s_{n} = \\frac{n}{n+1} \\nabla_{\\theta} a_{n-1} + \\frac{1}{n+1} \\nabla_{\\theta} s_{n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stock price $s_n$ at step $n$ is:\n",
    "$$s_n = s(s_{n-1}, \\theta) = s_{n-1} + \\frac{\\mu}{Y} s_{n-1} + \\sigma  \\sqrt{\\frac{1}{Y}} n_n s_{n-1}$$\n",
    "\n",
    "At the last time step $s_n$ is:\n",
    "$$s_n = s(s_{n-1}, \\theta) = s_{n-1} + \\frac{\\mu (T Y - n) }{Y} s_{n-1} + \\sigma  \\sqrt{\\frac{TY-n}{Y}} n_n s_{n-1}$$\n",
    "where the $n_n$ is the normal random number at step $n$ and we can treat them as constant.\n",
    "\n",
    "The gradient of $s_n$:\n",
    "$$\\nabla_{\\theta} s_n = \\nabla_{\\theta} s(s_{n-1}, \\theta) = (1 + \\frac{\\mu}{Y} + \\sigma \\sqrt{\\frac{1}{Y}} n_n) \\nabla_{\\theta} s_{n-1} + \\nabla_{\\theta} (1 + \\frac{\\mu}{Y} + \\sigma \\sqrt{\\frac{1}{Y}} n_n) s_{n-1} $$\n",
    "where the gradient in the second term is:\n",
    "$$\\nabla_{\\theta} (1 + \\frac{\\mu}{Y} + \\sigma \\sqrt{\\frac{1}{Y}} v_n) = (0, 0, 0, 1/Y, \\sqrt{1/Y} n_n ,0) $$\n",
    "\n",
    "The gradient of $s_n$ for the last step:\n",
    "$$\\nabla_{\\theta} s_n = \\nabla_{\\theta} s(s_{n-1}, \\theta) = (\\frac{1+ \\mu (T Y - n) }{Y} + \\sigma  \\sqrt{\\frac{TY-n}{Y}} n_n ) \\nabla_{\\theta} s_{n-1} + \\nabla_{\\theta} (\\frac{1+ \\mu (T Y - n) }{Y} + \\sigma  \\sqrt{\\frac{TY-n}{Y}} n_n )  s_{n-1} $$\n",
    "where the gradient in the second term is:\n",
    "$$\\nabla_{\\theta} (\\frac{1+ \\mu (T Y - n) }{Y} + \\sigma  \\sqrt{\\frac{TY-n}{Y}} n_n ) = (\\mu + \\frac{1}{2}\\sigma n_n (T-n/Y)^{-\\frac{1}{2}}, 0, 0, (T-n/Y), \\sqrt{T-n/Y} n_n ,0) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial contition $$\\nabla_{\\theta} S_0 = (0,0,1,0,0,0)$$\n",
    "\n",
    "Let's convert these equations into code in the Numba kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def numba_gpu_barrier_option(d_s, doutput, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS):\n",
    "    # ii - overall thread index\n",
    "    \n",
    "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
    "    tmp1 = mu/Y_STEPS\n",
    "    tmp2 = math.exp(-r*T)\n",
    "    tmp3 = math.sqrt(1.0/Y_STEPS)\n",
    "    running_average = 0.0\n",
    "    d_theta = numba.cuda.local.array(6, numba.float64)\n",
    "    d_a = numba.cuda.local.array(6, numba.float64)\n",
    "    for i in range(ii, N_PATHS, stride):\n",
    "        d_theta[0] = 0 # T\n",
    "        d_theta[1] = 0 # K\n",
    "        d_theta[2] = 1 # S_0\n",
    "        d_theta[3] = 0 # mu\n",
    "        d_theta[4] = 0 # sigma\n",
    "        d_theta[5] = 0 # r\n",
    "        for k in range(6):\n",
    "            d_a[k] = 0\n",
    "        s_curr = S0\n",
    "        for n in range(N_STEPS):\n",
    "            if n == N_STEPS - 1:\n",
    "                delta_t = T - n/Y_STEPS\n",
    "                tmp1 = delta_t * mu\n",
    "                tmp3 = math.sqrt(delta_t)  \n",
    "            \n",
    "            ## start to compute the gradient\n",
    "            factor = (1.0+tmp1+sigma*tmp3*d_normals[i + n * N_PATHS])\n",
    "            for k in range(6):\n",
    "                 d_theta[k] *= factor\n",
    "            if n == N_STEPS - 1:\n",
    "                d_theta[0] += (mu + 0.5 * sigma * d_normals[i + n * N_PATHS] / tmp3) * s_curr\n",
    "                d_theta[3] += (T - n/Y_STEPS) * s_curr\n",
    "                d_theta[4] += tmp3 * d_normals[i + n * N_PATHS] * s_curr\n",
    "            else:\n",
    "                d_theta[3] += 1.0/Y_STEPS * s_curr\n",
    "                d_theta[4] += tmp3 * d_normals[i + n * N_PATHS] * s_curr\n",
    "            for k in range(6):\n",
    "                d_a[k] = d_a[k]*n/(n+1.0) + d_theta[k]/(n+1.0)\n",
    "            ## start to compute current stock price and moving average\n",
    "              \n",
    "            s_curr += tmp1 * s_curr + sigma*s_curr*tmp3*d_normals[i + n * N_PATHS]\n",
    "            running_average += (s_curr - running_average) / (n + 1.0)\n",
    "            # print(running_average, n, tmp1 * s_curr, sigma,s_curr, tmp3,d_normals[i + n * N_PATHS])\n",
    "            if running_average <= B:\n",
    "                break\n",
    "        payoff = running_average - K if running_average>K else 0\n",
    "        d_s[i] = tmp2 * payoff\n",
    "        # gradient for strik \n",
    "        if running_average > K:\n",
    "            d_a[1] = -1\n",
    "            # adjust gradient for discount factor\n",
    "            for k in range(6):\n",
    "                d_a[k] *= tmp2\n",
    "            d_a[0] += payoff * tmp2* -r \n",
    "            d_a[5] += payoff * tmp2* -T\n",
    "        else:\n",
    "            for k in range(6):\n",
    "                d_a[k] = 0\n",
    "        for k in range(6):\n",
    "            doutput[k*N_PATHS+i] = d_a[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the simulation and benchmark the computation time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 1.4302270412445068 v 19.44895\n",
      "greeks [ -0.9348413   -0.66161776   0.76855695  52.945995    21.607492\n",
      " -21.393824  ]\n"
     ]
    }
   ],
   "source": [
    "number_of_threads = 256\n",
    "number_of_blocks = (N_PATHS-1) // number_of_threads + 1\n",
    "output = cupy.zeros(N_PATHS, dtype=cupy.float32)\n",
    "numba_gpu_barrier_option[(number_of_blocks,), (number_of_threads,)](output, doutput, np.float32(T), np.float32(K), \n",
    "                    np.float32(B), np.float32(S0), \n",
    "                    np.float32(sigma), np.float32(mu), \n",
    "                    np.float32(r), randoms_gpu, N_STEPS, N_PATHS)\n",
    "s = time.time()\n",
    "numba_gpu_barrier_option[(number_of_blocks,), (number_of_threads,)](output, doutput, np.float32(T), np.float32(K), \n",
    "                    np.float32(B), np.float32(S0), \n",
    "                    np.float32(sigma), np.float32(mu), \n",
    "                    np.float32(r), randoms_gpu, N_STEPS, N_PATHS)\n",
    "v = output.mean()\n",
    "cuda.synchronize()\n",
    "e = time.time()\n",
    "print('time', e-s, 'v', v)\n",
    "greeks = doutput.reshape(6, N_PATHS).mean(axis=1)\n",
    "print('greeks', greeks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we shown in the [developer blog](https://developer.nvidia.com/blog/accelerating-python-for-exotic-option-pricing/), Cupy implementation is faster as it compiles the native CUDA code. The following is the same GPU kernel that is implemented in Cupy. It can handle batches of simulations simutaneously in the GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "cupy_batched_barrier_option = cupy.RawKernel(r'''\n",
    "extern \"C\" __global__ void batched_barrier_option(\n",
    "    float *d_s,\n",
    "    float *d_d,\n",
    "    const float * T,\n",
    "    const float * K,\n",
    "    const float * B,\n",
    "    const float * S0,\n",
    "    const float * sigma,\n",
    "    const float * mu,\n",
    "    const float * r,\n",
    "    const float * d_normals,\n",
    "    const long *N_STEPS,\n",
    "    const long Y_STEPS,\n",
    "    const long N_PATHS,\n",
    "    const long N_BATCH)\n",
    "{\n",
    "  unsigned idx =  threadIdx.x + blockIdx.x * blockDim.x;\n",
    "  unsigned stride = blockDim.x * gridDim.x;\n",
    "  unsigned tid = threadIdx.x;\n",
    "  double d_theta[6];\n",
    "  double d_a[6];\n",
    "\n",
    "  for (unsigned i = idx; i<N_PATHS * N_BATCH; i+=stride)\n",
    "  {\n",
    "    d_theta[0] = 0; // T\n",
    "    d_theta[1] = 0; // K\n",
    "    d_theta[2] = 1.0; // S_0\n",
    "    d_theta[3] = 0; // mu\n",
    "    d_theta[4] = 0; // sigma\n",
    "    d_theta[5] = 0; // r\n",
    "    for (unsigned k = 0; k < 6; k++){\n",
    "      d_a[k] = 0.0;\n",
    "    }\n",
    "    \n",
    "    int batch_id = i / N_PATHS;\n",
    "    int path_id = i % N_PATHS;\n",
    "    float s_curr = S0[batch_id];\n",
    "    float tmp1 = mu[batch_id]/Y_STEPS;\n",
    "    float tmp2 = exp(-r[batch_id]*T[batch_id]);\n",
    "    float tmp3 = sqrt(1.0/Y_STEPS);\n",
    "    unsigned n=0;\n",
    "    double running_average = 0.0;\n",
    "    for(unsigned n = 0; n < N_STEPS[batch_id]; n++){\n",
    "        if (n == N_STEPS[batch_id] - 1) {\n",
    "            float delta_t = T[batch_id] - n/Y_STEPS;\n",
    "            tmp1 = delta_t * mu[batch_id];\n",
    "            tmp3 = sqrt(abs(delta_t));\n",
    "        }\n",
    "        float normal = d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH];\n",
    "        \n",
    "            \n",
    "        // start to compute the gradient\n",
    "        float factor = (1.0+tmp1+sigma[batch_id]*tmp3*normal);\n",
    "        for (unsigned k=0; k < 6; k++) {\n",
    "            d_theta[k] *= factor;\n",
    "        }\n",
    "        \n",
    "        if (n == N_STEPS[batch_id] - 1){\n",
    "                d_theta[0] += (mu[batch_id] + 0.5 * sigma[batch_id] * normal / tmp3) * s_curr;\n",
    "                d_theta[3] += (T[batch_id] - n/Y_STEPS) * s_curr;\n",
    "                d_theta[4] += tmp3 * normal * s_curr;\n",
    "        }\n",
    "        else {\n",
    "                d_theta[3] += 1.0/Y_STEPS * s_curr;\n",
    "                d_theta[4] += tmp3 * normal * s_curr;\n",
    "        }\n",
    "        for (unsigned k = 0; k < 6; k++) {\n",
    "                d_a[k] = d_a[k]*n/(n+1.0) + d_theta[k]/(n+1.0); \n",
    "        }\n",
    "        \n",
    "        \n",
    "        // start to compute current stock price and moving average       \n",
    "       \n",
    "       s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*normal;\n",
    "       running_average += (s_curr - running_average) / (n + 1.0);\n",
    "       if (running_average <= B[batch_id]){\n",
    "           break;\n",
    "       }\n",
    "    }\n",
    "\n",
    "    float payoff = (running_average>K[batch_id] ? running_average-K[batch_id] : 0.f); \n",
    "    d_s[i] = tmp2 * payoff;\n",
    "    //printf(\"%d, %d, %f, %f, %f, %d\\n\", i, idx, d_s[i], payoff, K[batch_id], batch_id);\n",
    "    \n",
    "    // gradient for strik \n",
    "    if (running_average > K[batch_id]){\n",
    "       d_a[1] = -1.0;\n",
    "       // adjust gradient for discount factor\n",
    "       for (unsigned k = 0; k < 6; k++) {\n",
    "            d_a[k] *= tmp2;\n",
    "        }\n",
    "        d_a[0] += payoff * tmp2* -r[batch_id];\n",
    "        d_a[5] += payoff * tmp2* -T[batch_id];\n",
    "        \n",
    "    }\n",
    "    else {\n",
    "        for (unsigned k = 0; k < 6; k++) {\n",
    "           d_a[k] = 0.0;\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    for (unsigned k = 0; k < 6; k++) {\n",
    "       d_d[k*N_PATHS*N_BATCH+i] = d_a[k];\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "''', 'batched_barrier_option')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the driver function into a function to call this Cupy GPU kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 0.4407541751861572 v [19.486385   1.3048422]\n",
      "(6, 2)\n",
      "gradient [[ -0.93805814  -0.05804303]\n",
      " [ -0.66064537  -0.14803363]\n",
      " [  0.76797765   0.19068879]\n",
      " [ 52.938847    12.240291  ]\n",
      " [ 21.815239    17.849463  ]\n",
      " [-21.435024    -1.5658108 ]]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "Y_STEPS = 252\n",
    "N_BATCH = 2\n",
    "N_PATHS = 102400\n",
    "K = cupy.array([110.0, 120.0], dtype=cupy.float32)\n",
    "B = cupy.array([100.0, 90.0], dtype=cupy.float32)\n",
    "S0 = cupy.array([120.0, 100.0], dtype=cupy.float32)\n",
    "sigma = cupy.array([0.35, 0.2], dtype=cupy.float32)\n",
    "mu = cupy.array([0.1, 0.1], dtype=cupy.float32)\n",
    "r =cupy.array([0.05, 0.05], dtype=cupy.float32)\n",
    "T =cupy.array([1.1, 1.2], dtype=cupy.float32)\n",
    "N_STEPS = cupy.ceil(T * Y_STEPS).astype(cupy.int64)\n",
    "\n",
    "\n",
    "def batch_run():\n",
    "    number_of_threads = 256\n",
    "    number_of_blocks = (N_PATHS * N_BATCH - 1) // number_of_threads + 1\n",
    "    random_elements = (N_STEPS.max()*N_PATHS*N_BATCH).item()\n",
    "    randoms_gpu = cupy.random.normal(0, 1, random_elements, dtype=cupy.float32)\n",
    "    output = cupy.zeros(N_BATCH*N_PATHS, dtype=cupy.float32)\n",
    "    d_output = cupy.zeros(N_BATCH*N_PATHS*6, dtype=cupy.float32)\n",
    "    cupy.cuda.stream.get_current_stream().synchronize()\n",
    "    s = time.time() \n",
    "    cupy_batched_barrier_option((number_of_blocks,), (number_of_threads,),\n",
    "                       (output, d_output, T, K, B, S0, sigma, mu, r,\n",
    "                        randoms_gpu, N_STEPS, Y_STEPS, N_PATHS, N_BATCH))\n",
    "    v = output.reshape(N_BATCH, N_PATHS).mean(axis=1)\n",
    "    b = d_output.reshape(6, N_BATCH, N_PATHS).mean(axis=2)\n",
    "    cupy.cuda.stream.get_current_stream().synchronize()\n",
    "    e = time.time()\n",
    "    print('time', e-s, 'v',v)\n",
    "    print(b.shape)\n",
    "    print('gradient', b)\n",
    "    return output\n",
    "o = batch_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the simulation, it generates both option price and Greeks. gQuant organizes all the computation steps into weakly coupled computation nodes.\n",
    "\n",
    "We create one node that is used to generate random Option parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c10dc83e8234ac3a388cf7a669a0296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GQuantWidget(sub=HBox(), value=[OrderedDict([('id', 'parameters'), ('type', 'ParaNode'), ('conf', {'seed': Non…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gquant.dataframe_flow import TaskGraph\n",
    "taskgraph = TaskGraph.load_taskgraph('../taskgraphs/option_price_example/option_parameter.gq.yaml')\n",
    "taskgraph.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paramter node returns an iteratable object that emits one set of parameters at a time. Let's evaluate it and get 3 parameters samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[NeMo W 2020-10-13 17:41:09 audio_preprocessing:56] Could not import torchaudio. Some features might not work.\n",
      "[NeMo W 2020-10-13 17:41:09 audio_preprocessing:61] Unable to import APEX. Mixed precision and distributed training will not work.\n",
      "[NeMo W 2020-10-13 17:41:12 transformer_modules:36] Unable to import FusedLayerNorm  from APEX. Using regular LayerNorm instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: APEX is not installed, multi_tensor_applier will not be available.\n",
      "WARNING: APEX is not installed, using torch.nn.LayerNorm instead of apex.normalization.FusedLayerNorm!\n",
      "[[1.28891945e+01 7.15959847e-01 1.73863697e+01 1.24069878e+02\n",
      "  1.93033934e-01 1.15646943e-01 1.07296132e-01]]\n",
      "[[6.1168404e+01 1.0110656e+00 9.4205116e+01 7.3750069e+01 2.3799250e-02\n",
      "  9.0849556e-02 3.4394655e-02]]\n",
      "[[ 16.547909     1.7067083   36.221733   127.37139      0.19490878\n",
      "    0.13936971   0.17518456]]\n"
     ]
    }
   ],
   "source": [
    "para_iter = taskgraph.run()[0]\n",
    "print(next(para_iter))\n",
    "print(next(para_iter))\n",
    "print(next(para_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter node feed the parameter iterator to simulaiton node that computes the option price and greeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3cf241a3174e63971bcde3ece1762c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GQuantWidget(sub=HBox(), value=[OrderedDict([('id', 'parameters'), ('type', 'ParaNode'), ('conf', {'seed': Non…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gquant.dataframe_flow import TaskGraph\n",
    "taskgraph = TaskGraph.load_taskgraph('../taskgraphs/option_price_example/option_simulation.gq.yaml')\n",
    "taskgraph.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulation node returns an iteratable object that emits the parameters alongside with the corresponding option price and Greeks. Let's evaluate it and run 3 simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[1.1642005e+02, 1.0885608e+00, 1.7363400e+02, 4.7797112e+01,\n",
      "        3.6089797e-02, 3.9603639e-01, 1.8389491e-02]], dtype=float32), array([[0., 0., 0., 0., 0., 0., 0.]], dtype=float32))\n",
      "(array([[1.6963057e+01, 1.3985612e+00, 1.4333043e+02, 2.0029900e+02,\n",
      "        1.6011228e-01, 4.7889765e-02, 6.9490828e-02]], dtype=float32), array([[ 7.3783440e+01, -5.0241566e+00, -9.0738451e-01,  1.0176760e+00,\n",
      "         1.4869339e+02, -4.8342802e-02, -1.0319065e+02]], dtype=float32))\n",
      "(array([[1.36661367e+01, 1.77841282e+00, 2.43063908e+01, 1.05191399e+02,\n",
      "        1.95957035e-01, 1.03947945e-01, 3.80742587e-02]], dtype=float32), array([[ 9.50382614e+01, -3.55774021e+00, -9.34528708e-01,\n",
      "         1.11941957e+00,  1.11425446e+02,  1.44090921e-01,\n",
      "        -1.69017273e+02]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "sim_iter = taskgraph.run()[0]\n",
    "print(next(sim_iter))\n",
    "print(next(sim_iter))\n",
    "print(next(sim_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulation computes both the option price and Greeks. Greeks can be used to add differential regularization to the cost function. The simulation is very costly even with GPUs. The Option price accuracy depends on the number of paths because the standard deviation scale with the $n$ number of paths as $\\frac{1}{\\sqrt{n}}$. To speed up the option pricing and Greek compuation, we can use a neural network to approximate the option pricing simulation. \n",
    "\n",
    "We have seen the simulation can generate any numbers of data points in Cupy GPU arrays. It is easy to convert Cupy GPU array into Pytorch tensors via DLpack library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.9171e+01, 1.6421e+00, 1.7739e+02, 1.9516e+02, 1.5127e-01, 1.8999e-01,\n",
      "        7.6484e-03], device='cuda:0')\n",
      "tensor([ 44.2958,  -0.2524,  -0.9181,   1.0615, 178.5863,  16.7144, -72.7383],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.dlpack import from_dlpack\n",
    "X, Y = next(sim_iter)\n",
    "X_t, Y_t = (from_dlpack(X[0].toDlpack()), from_dlpack(Y[0].toDlpack()))\n",
    "print(X_t)\n",
    "print(Y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to move from HPC to Deep Learning world. We create a gQuant node that takes an iterator of simulation results and convert it into NeMo DataLayer. We use the basic feed foward neural network to approximate the option prices. But we choose to use `Elu` activation function as we need high order differenations. While the popular `ReLu` activation funciton only have non-zero first order differentiation. \n",
    "\n",
    "Pytorch provides `grad` method to compute the gradient of the inputs. For a batch of input data points, we would like to calcuate all the input gradients in one step. The trick is to sum the batch of outputs together. Here is an example to compute the gradient of a batch of inputs for the function $f(x, y) = (xy)^2$. It's gradient is $$\\nabla f(x,y) = (2 x^2 y, 2 y^2 x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 24.,  36.],\n",
      "        [200., 160.]], grad_fn=<DivBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "'''\n",
    "z = (xy)^2\n",
    "x = 3, y =2\n",
    "first order deriv [24 36]\n",
    "x = 4, y =5\n",
    "first order deriv [200, 160]\n",
    "'''\n",
    "# create a batch of two inputs\n",
    "inputs = torch.tensor([[3.0,2.0], [4.0, 5.0]], requires_grad=True)\n",
    "# sum the outputs together\n",
    "z = (inputs.prod(axis=1)**2).sum()\n",
    "first_order_grad = grad(z, inputs, create_graph=True)\n",
    "print(first_order_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the feed-forward network, we use this trick to compute the gradients for all the input data points. The loss function will be composed of two terms. One is the regression on the option price prediction, and the second one is the regularization term to make sure the gradients match.\n",
    "\n",
    "load the gquant task graph to train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ecf870404a6460a8e7cd6e32f5c173e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GQuantWidget(sub=HBox(), value=[OrderedDict([('id', 'parameters'), ('type', 'ParaNode'), ('conf', {'seed': Non…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numba\n",
    "from numba import cuda\n",
    "import nemo\n",
    "from gquant.dataframe_flow import TaskGraph\n",
    "\n",
    "nemo.core.NeuralModuleFactory()\n",
    "taskgraph = TaskGraph.load_taskgraph('../taskgraphs/option_price_example/option_price_nemo.gq.yaml')\n",
    "taskgraph.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the `run` button to see the training in effect. Click on the `show log` button to see the full logs about the training loss descreasing.\n",
    "\n",
    "Since we have all the gQuant nodes handy for the simulation and deep learning, we can re-use them to construct the evaluation network. The evaluation network will reuse the same neural network in the training to evaluate the prediction in a seperate dataset. In the train node, we can monitor the evaluation performance to determine whether the model has good generalization or not. The following is the full network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554c29a5500f430c8a07f60b4f14de44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GQuantWidget(sub=HBox(), value=[OrderedDict([('id', 'parameters'), ('type', 'ParaNode'), ('conf', {'seed': Non…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "taskgraph = TaskGraph.load_taskgraph('../taskgraphs/option_price_example/full_training_model.gq.yaml')\n",
    "taskgraph.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, we can start to run the inference on the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d003af5e8514a4fa2bfdf64aac52b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GQuantWidget(sub=HBox(), value=[OrderedDict([('id', 'parameters'), ('type', 'ParaNode'), ('conf', {'seed': Non…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "taskgraph = TaskGraph.load_taskgraph('../taskgraphs/option_price_example/option_price_inference.gq.yaml')\n",
    "taskgraph.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
